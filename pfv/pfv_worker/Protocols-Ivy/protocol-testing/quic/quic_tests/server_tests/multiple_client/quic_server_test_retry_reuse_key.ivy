#lang ivy1.7

include order
include quic_infer
include file
include ivy_quic_shim_client
include quic_locale
include ivy_quic_n_clients_behavior

#
# We fix the initial transport parameters here. TODO: In principle these
# parameters should be selected randomly by some event.
#

include ivy_quic_client_standard_tp

# Network setup
# -------------
#
# To test the server over the OS sockets layer, we need some setup. We must
# establish which interface and which ports the tester will use. 
#

# Mandatory for longbv else truncated

var the_cid_alt : cid 
var server_cid_alt : cid

after init {
    # the_cid := 0x1; # 0x0;
    # server_cid := 0x2; # 0x1;
    the_cid_alt := 0x3; # 0x0;
    server_cid_alt := 0x4; # 0x1;
}    

after init {
    client_vn := endpoint_id_addr(endpoint_id.client_vn);
}

# This creates the sockets and also a TLS instance to be used by `the_cid`.
# TODO: the TLS instance should be created dynamically when the application
# layer creates a connection. 

# https://github.com/private-octopus/picoquic/issues/923

# import second_client
# import second_server

after init {
    sock := net.open(endpoint_id.client,client.ep);
    sock_alt := net.open(endpoint_id.client_alt,second_client.ep);
    sock_vn := sock; #net.open(endpclientoint_id.client_vn,client_vn);
    client.set_tls_id(0);
    second_client.set_tls_id(2); # TODO
    # call clients.init_tls_ids;
    server.set_tls_id(1);
    second_server.set_tls_id(3);
    # call servers.init_tls_ids;
    var extns := tls_extensions.empty;
    extns := extns.append(make_transport_parameters);
    call tls_api.upper.create(0,false,extns);  # false means this instance of tls is not a server
}

after init {
    var extns2 := tls_extensions.empty;
    extns2 := extns2.append(make_transport_parameters_c2);
    #call tls_api.upper.create_no_hs(2,false,extns2);  # false means this instance of tls is not a server
    call tls_api.upper.create(2,false,extns2);  # false means this instance of tls is not a server
}

#
# We fix the initial transport parameters here. TODO: In principle these
# parameters should be selected randomly by some event.
#

action make_transport_parameters_c2 returns (tp:quic_transport_parameters) = {
    var imsdbl : initial_max_stream_data_bidi_local;
    imsdbl.stream_pos_32 := max_stream_data;
    var imd : initial_max_data;
    imd.stream_pos_32 := random_stream_pos(1,0xFFFFFFF);
    var it : max_idle_timeout;
    it.seconds_16 := random_microsecs(0,0xFFFFFFF);
    var imsdbr : initial_max_stream_data_bidi_remote;
    imsdbr.stream_pos_32 := max_stream_data;
    var imsdu : initial_max_stream_data_uni;
    imsdu.stream_pos_32 := random_stream_pos(1,0xFFFFFFF);
    tp.transport_parameters := tp.transport_parameters.append(imsdbl);
    tp.transport_parameters := tp.transport_parameters.append(imd);
    tp.transport_parameters := tp.transport_parameters.append(it);
    tp.transport_parameters := tp.transport_parameters.append(imsdbr);
    tp.transport_parameters := tp.transport_parameters.append(imsdu);

    var icid : initial_source_connection_id; #TODO update v29
    icid.scid := the_cid_alt;
    tp.transport_parameters := tp.transport_parameters.append(icid);

    var no_migration : disable_active_migration; #TODO update v29
    tp.transport_parameters := tp.transport_parameters.append(no_migration);

    var max_streams_bidi : initial_max_stream_id_bidi; #TODO update v29
    max_streams_bidi.stream_id_16 := random_stream_id(4,0xFFFFFFF);
    tp.transport_parameters := tp.transport_parameters.append(max_streams_bidi);
}

after init {
	allowed_multiple_migration := false;
    allowed_migration := false;
    version_negociated := false;
    # initial_version := 0xff00001c;
}

after init {
    # supported_versions := versions.empty;
    # var v1 := stream_data.empty;
    # v1 := v1.append(0xff);
    # v1 := v1.append(0x0);
    # v1 := v1.append(0x0);
    # v1 := v1.append(0x1c); # TODO
    # supported_versions := supported_versions.append(v1);

    # supported_versions_bv := versions_bv.empty;
    # supported_versions_bv := supported_versions_bv.append(0xff00001c);

    token_saved := false;
}

# This token MUST be repeated by the client in all
#    Initial packets it sends for that connection after it receives the
#    Retry packet. 

# In comparison, a
#    token obtained in a Retry packet MUST be used immediately during the
#    connection attempt and cannot be used in subsequent connection
#    attempts.

# A client MUST
#    discard a Retry packet that contains a Source Connection ID field
#    that is identical to the Destination Connection ID field of its
#    Initial packet TODO

# The client MUST use the value from the Source
#    Connection ID field of the Retry packet in the Destination Connection
#    ID field of subsequent packets that it sends.

before packet_event(src:ip.endpoint,dst:ip.endpoint,pkt:quic_packet) {
    if _generating {
        require src = client.ep | src = second_client.ep;
        #require (pkt.ptype = quic_packet_type.initial & src = client.ep -> pkt.dst_cid = server_cid) & (pkt.ptype = quic_packet_type.initial & src = second_client.ep -> pkt.dst_cid = server_cid_alt);
        require (pkt.ptype = quic_packet_type.initial & pkt.dst_cid = server_cid -> src = client.ep) & (pkt.ptype = quic_packet_type.initial &  pkt.dst_cid = server_cid_alt -> src = second_client.ep);
        require (src = client.ep <-> pkt.dst_cid = connected_to(the_cid)) | (src = second_client.ep <-> pkt.dst_cid = connected_to(the_cid_alt));

        # if src = second_client.ep {
        #     if pkt.ptype = quic_packet_type.initial {
        #         pkt.dst_cid := server_cid_alt; # TODO
        #     };
        # } else {
        #     require src = client.ep;  # if src = client.ep 
        #     if pkt.ptype = quic_packet_type.initial {
        #         pkt.dst_cid := server_cid; # TODO
        #     };
        # };
        require dst = server.ep;
        if pkt.ptype = quic_packet_type.initial { # & retry_recv & ~zero_length_token
            # var expected_token := tls_api.upper.get_old_retry_token(0);
            # call show_token_retry(expected_token);
            # #require pkt.token.end = expected_token.end;
            # pkt.token := expected_token;
            #require pkt.token = retry_token_recv;
            # if src = second_client.ep {
            #     var expected_token := tls_api.upper.get_old_new_token(2); ## TODO RETRY token ??
            #     pkt.token := expected_token;
            # } else {
            #     var expected_token := tls_api.upper.get_old_new_token(0);
            #     pkt.token := expected_token;
            # };
            var expected_token := tls_api.upper.get_old_retry_token;
            pkt.token := expected_token;
        } else {
            require pkt.token.end = 0; 
        };
    };
    if final_version = 0x00000000 {
        #require pkt.long -> pkt.pversion = 0x00000001  #version 29
        #require pkt.long -> pkt.pversion = 0xff000020  #version 32
        #require pkt.long -> pkt.pversion = 0xff00001e  #version 30
        require pkt.long -> pkt.pversion = initial_version  #version 30
    } else {
        require pkt.long -> pkt.pversion = final_version
    };
}
import action show_token_retry(v:stream_data)

# The protocol specification describes all the events occurring the
# system at all protocol layers. When constructing the test mirror,
# however, we need to generate only events that are outputs of the
# environment. Ivy doesn't have any built-in mechanism to do this,
# so we have to add a constraint to every generated action. 
#
# We also add other pre-conditions to the actions to try to restrict
# them to relevant parameter values. 
#
# To restrict the generated events, we use the built-in predicate
# "_generating" that is true if this is a generated event.
#
# This is the mirror constraint for the stream frame protocol. We add
# some requirements to make the tests more interesting. In particular,
# we don't want the tester to produce lots of tiny frames (or empty
# ones) so we require that a stream frame send all of the available
# stream data. 
#
# TODO: We reuquire tje `off` and `len` bits to be true. Maybe these should
# determined in the low-level packet encoding stage.

before frame.stream.handle(f:frame.stream,scid:cid,dcid:cid,e:quic_packet_type) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require (connected(the_cid) & dcid = connected_to(the_cid)) | (connected(the_cid_alt) & dcid = connected_to(the_cid_alt));
        require f.len & f.off; 
        require f.length > 0;
        require f.offset = stream_length(dcid,f.id);
        require f.length = (stream_app_data_end(dcid,f.id)) - f.offset;
    };

}

# We have something similar for crypto frames

before frame.crypto.handle(f:frame.crypto,scid:cid,dcid:cid,e:quic_packet_type) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require f.length > 0;
        call show_crypto_length(crypto_length(scid,e));
        if e = quic_packet_type.initial & false {
            require f.offset = 0;
        } else {
            require f.offset = crypto_length(scid,e);
        };
        require f.length = (crypto_data_end(scid,e)) - f.offset;
    }
}



# Generate ack frames only for the client. HACK: to avoid generating
# blizzards of ACK's, we set the bit `force_new_ack` which forces
# ACK's to have at least on previously unacked sequenc number. There
# should be a better way to do this.

before frame.ack.handle(f:frame.ack,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        force_new_ack := true;
        #call handle_sending_ack(f.largest_acked);
    } 
}

# Generate rst_stream frames only for the environment process(es).

before frame.rst_stream.handle(f:frame.rst_stream,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require f.id = 4;
    }
}

# Generate stop_sending frames only for the environment process(es).

before frame.stop_sending.handle(f:frame.stop_sending,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require f.id = 4;
    }
}

# Generate max_streams frames only for the environment process(es).

before frame.max_streams.handle(f:frame.max_streams,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
    }
}

before frame.new_connection_id.handle(f:frame.new_connection_id,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
    }
}


before frame.retire_connection_id.handle(f:frame.retire_connection_id,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
    }
}


# Generate connection_close frames only for the environment process(es).
#
# Note: requiring the `err_code` is zero on non-generated frames means that
# we stop the test if the peer reports a protocol error. 

before frame.connection_close.handle(f:frame.connection_close,scid:cid,dcid:cid) {
    if _generating {
        require (scid = the_cid & conn_total_data(the_cid) > 100) & (scid = the_cid_alt & conn_total_data(the_cid_alt) > 100);
        require f.err_code = 0;
    } else {
        require is_no_error;
        call _finalize;
    }
}

# Generate application_close frames only for the environment process(es).

before frame.application_close.handle(f:frame.application_close,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require (conn_total_data(the_cid) > 30) & (conn_total_data(the_cid_alt) > 30); # tricks
        require f.err_code = 0;
    } else {
        require is_no_error;
        call _finalize;
    }
}

# Generate max_stream_data frames only for the environment process(es), and only for
# a restricted range of frames (so we don't throttle all of the data).

before frame.max_stream_data.handle(f:frame.max_stream_data,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require 4 <= f.id & f.id <= 16;
    }
}

# Generate stream_data_blocked frames only for the environment process(es).
# We restrict the frame id.

before frame.stream_data_blocked.handle(f:frame.stream_data_blocked,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require f.id = 4;
    }
}

# Generate max_data frames only for the environment process(es). Here, for
# non-generated frames, we check that the max_data value doesn't decrease.
# This is not forbidden, but it seems like a good thing to check.

before frame.max_data.handle(f:frame.max_data,scid:cid,dcid:cid) {
    if _generating {
        require scid = the_cid | scid = the_cid_alt;
        require f.pos = 4000;
    }
    else {
        require ~(max_data_set(scid) & max_data_val(scid) > f.pos); #TODO
        var tp := trans_params(scid);
        if initial_max_data.is_set(tp) {
# TEMORARY: commented out until fixed
#            require initial_max_data.value(tp).stream_pos_32 <= f.pos;
        }
    }
}

# This is where we restrict the client to send only a specific HTTP
# request.  We allow generating application data at the client end
# only if the previous data have not been sent. This is to avoid
# building up a long queue. We only send the http request data read
# from the input file, up to the first newline. We restrict the client
# to use stream id's from 4 to 60 in increments of 4.

after init {
    current_stream := 4
}

around client_send_event {
    require s = current_stream;
    #require (pkt.dst_cid = server_cid -> src = client.ep) & (pkt.dst_cid = server_cid_alt -> src = second_client.ep);
    require src = client.ep | src = second_client.ep;
    require dst = server.ep;
    require ((connected(the_cid) & dcid = connected_to(the_cid))) | (connected(the_cid_alt) & dcid = connected_to(the_cid_alt));
    #require (dcid = connected_to(the_cid) -> src = client.ep) & (dcid =  connected_to(the_cid_alt) -> src = second_client.ep);
    require (src = client.ep <-> dcid = connected_to(the_cid)) | (src = second_client.ep <-> dcid = connected_to(the_cid_alt));
    require stream_length(dcid,s) = stream_app_data_end(dcid,s);
    require stream_length(dcid,s) < end & end <= http_request.end;
    require current_stream < 120;
    ...
    while end < http_request.end & http_request.value(end) ~= 10 {
        end := end.next
    };
    if end < http_request.end {
        end := end.next
    };
    var data := http_request.segment(stream_length(dcid,s),end);
#    call app_send_event(src,dst,dcid,s,data,end = http_request.end);
    call app_send_event(src,dst,dcid,s,data,stream_length(dcid,s),true);
    current_stream := current_stream + 4;
}

# The actions listed below will be generated by the mirror.
#
# Note: some of these are commented out. They are added in other files
# that include this one.

export frame.ack.handle
export frame.stream.handle
export frame.crypto.handle
export frame.path_response.handle
export packet_event
export client_send_event
export tls_recv_event
#export frame.new_connection_id.handle
#export frame.retire_connection_id.handle
#export frame.rst_stream.handle
#export frame.max_stream_id.handle
export frame.connection_close.handle
#export frame.max_stream_data.handle
#export frame.max_data.handle
attribute frame.crypto.handle.weight = "5"
attribute frame.path_response.handle.weight = "5"

# Final check
#
# When the test is complete, the tester calls the special action `_finalize`.
# We use this action to make some heuristic checks, for example that some
# data was actually received from the server. We can add advice to this
# action with additional checks.

export action _finalize = {
    # chris TODO 
    require conn_total_data(the_cid) > 0;
    require conn_total_data(the_cid_alt) > 0;
}

